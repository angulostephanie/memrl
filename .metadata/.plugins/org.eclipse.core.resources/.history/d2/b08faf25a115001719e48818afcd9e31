package myProj;

import java.util.ArrayList;
import java.util.List;
import java.util.Random;

import burlap.behavior.singleagent.Episode;
import burlap.behavior.singleagent.auxiliary.EpisodeSequenceVisualizer; 
import burlap.behavior.singleagent.learning.LearningAgent;
import burlap.behavior.singleagent.learning.tdmethods.QLearning;
import burlap.domain.singleagent.gridworld.GridWorldDomain;
import burlap.domain.singleagent.gridworld.GridWorldTerminalFunction;
import burlap.domain.singleagent.gridworld.GridWorldRewardFunction;
import burlap.domain.singleagent.gridworld.GridWorldVisualizer;
import burlap.domain.singleagent.gridworld.state.GridAgent;
import burlap.domain.singleagent.gridworld.state.GridLocation;
import burlap.domain.singleagent.gridworld.state.GridWorldState;
import burlap.mdp.singleagent.environment.SimulatedEnvironment;
import burlap.mdp.core.state.State;
import burlap.mdp.singleagent.SADomain;
import burlap.shell.visual.VisualExplorer;
import burlap.statehashing.simple.SimpleHashableStateFactory;
import burlap.visualizer.Visualizer;

public class TMazeGrid {
	public int c; 
	
	public static void main(String[] args) {
		TMazeGrid test = new TMazeGrid();
		test.QLearning();
	}
	 
	public void QLearning() {
	
		GridWorldDomain tmaze = new GridWorldDomain(3,2); //3x2 grid world
		GridWorldTerminalFunction endStates = new GridWorldTerminalFunction();
		GridWorldRewardFunction rf = new GridWorldRewardFunction(3,2,-1);
		
		
		tmaze.verticalWall(1, 1, 0); //left wall
		tmaze.verticalWall(1, 1, 2); //right wall
		tmaze.setNumberOfLocationTypes(2);
		
		rf.setReward(0, 0, 10); // (x, y, reward)
		
		endStates.markAsTerminalPosition(0, 0); // A 
		endStates.markAsTerminalPosition(2, 0); // C
		
		tmaze.setTf(endStates);
		tmaze.setRf(rf);
		
		
		SADomain domain = tmaze.generateDomain(); //generate the grid world domain
		
		//setup initial state
		State s = new GridWorldState(new GridAgent(1, 1)); //still need to toggle blue state (reward placement == gridlocation var)
		
		SimulatedEnvironment env = new SimulatedEnvironment(domain, s);
		
		//create Q-learning (state, discount factor,matrix of memory,qInit,learning rate)
		LearningAgent agent = new QLearning(domain, 0.7, new SimpleHashableStateFactory(), 
		0.1, 0.9); 
		
		//run Q-learning and store results in a list
		List<Episode> episodes = new ArrayList<Episode>(1000);
		for(int i = 0; i < 10; i++){
			//System.out.println(c);
			System.out.println("EPISODE #" + i);
			Episode ea = agent.runLearningEpisode(env);
			episodes.add(ea);
			for(int j = 0; j < ea.numTimeSteps(); j++){
				System.out.println("STEP #" + j);
				if(j == 0) {
					System.out.println("Action: " + ea.action(j));
					System.out.println("State: " + ea.state(j));
				} else if (j >= (ea.numTimeSteps()) ) {
					System.out.println("Action: " + ea.action(j));
					System.out.println("State: " + ea.state(j));
					System.out.println("Reward: " + ea.reward(j));
				} else {
					System.out.println("State: " + ea.state(j));
					System.out.println("Reward: " + ea.reward(j));
				}
			}
			env.resetEnvironment();
		}
		
		//create visualizer and explorer
		Visualizer v = GridWorldVisualizer.getVisualizer(tmaze.getMap());
		VisualExplorer exp = new VisualExplorer(domain, v, s);
		new EpisodeSequenceVisualizer(v, domain, episodes);
		
		exp.initGUI();
	}
	
	public int toggleRewardLocation() {
		Random random = new Random();
		int a = 0;
		int b = 2;
		c = random.nextBoolean() ? a:b;
		return c;
	}


}
