package myProj;

import java.util.ArrayList;
import java.util.List;
import java.util.Random;

import burlap.behavior.singleagent.Episode;
import burlap.behavior.singleagent.auxiliary.EpisodeSequenceVisualizer; 
import burlap.behavior.singleagent.learning.LearningAgent;
import burlap.behavior.singleagent.learning.tdmethods.QLearning;
import burlap.domain.singleagent.gridworld.GridWorldDomain;
import burlap.domain.singleagent.gridworld.GridWorldTerminalFunction;
import burlap.domain.singleagent.gridworld.GridWorldRewardFunction;
import burlap.domain.singleagent.gridworld.GridWorldVisualizer;
import burlap.domain.singleagent.gridworld.state.GridAgent;
import burlap.domain.singleagent.gridworld.state.GridLocation;
import burlap.domain.singleagent.gridworld.state.GridWorldState;
import burlap.mdp.singleagent.environment.SimulatedEnvironment;
import burlap.mdp.core.state.State;
import burlap.mdp.singleagent.SADomain;
import burlap.shell.visual.VisualExplorer;
import burlap.statehashing.simple.SimpleHashableStateFactory;
import burlap.visualizer.Visualizer;

public class TMazeGrid {
	
	public static void main(String[] args) {
		QLearning();
	}
	 
	public static void QLearning() {
	
		GridWorldDomain tmaze = new GridWorldDomain(3,2); //3x2 grid world
		GridWorldTerminalFunction endStates = new GridWorldTerminalFunction();
		GridWorldRewardFunction rf = new GridWorldRewardFunction(3,2);
		rf.setReward(0, 0, 10);
		endStates.markAsTerminalPosition(0, 0); // A 
		endStates.markAsTerminalPosition(2, 0); // C
		
		tmaze.verticalWall(1, 1, 0); //left wall
		tmaze.verticalWall(1, 1, 2); //right wall
		//tmaze.setProbSucceedTransitionDynamics(0.8);
		
		tmaze.setTf(endStates);
		tmaze.setRf(rf);
		
		SADomain domain = tmaze.generateDomain(); //generate the grid world domain
		
		//setup initial state
		State s = new GridWorldState(new GridAgent(1, 1)); //need to toggle blue state
		
		SimulatedEnvironment env = new SimulatedEnvironment(domain, s);
		
		//create Q-learning (state, discount factor,matrix of memory,qInit,learning rate)
		LearningAgent agent = new QLearning(domain, 0.7, new SimpleHashableStateFactory(), 
		0.1, 0.9); 
		
		//run Q-learning and store results in a list
		List<Episode> episodes = new ArrayList<Episode>(100);
		for(int i = 0; i < 5; i++){
			//System.out.println(c);
			Episode ea = agent.runLearningEpisode(env);
			episodes.add(ea);
			for(int j = 0; j < ea.numTimeSteps()-1; j++){
				System.out.println("Action: " + ea.action(j) + " at time " + j);
				System.out.println("State: " + ea.state(j) + " at time " + j );
				System.out.println("Reward: " + ea.reward(j) + " at time " + j);
			}
			//System.out.println(env.currentObservation());
			env.resetEnvironment();
		}
		
		//create visualizer and explorer
		Visualizer v = GridWorldVisualizer.getVisualizer(tmaze.getMap());
		VisualExplorer exp = new VisualExplorer(domain, v, s);
		new EpisodeSequenceVisualizer(v, domain, episodes);
		
		exp.initGUI();
	}
	
	public int toggleRewardLocation() {
		Random random = new Random();
		int a = 0;
		int b = 2;
		int c = random.nextBoolean() ? a:b;
		return c;
	}


}
