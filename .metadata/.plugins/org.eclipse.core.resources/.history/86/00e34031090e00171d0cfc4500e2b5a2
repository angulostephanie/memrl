package myProj;

import java.util.ArrayList;
import java.util.List;

import burlap.behavior.singleagent.Episode;
import burlap.behavior.singleagent.auxiliary.EpisodeSequenceVisualizer;
import burlap.behavior.singleagent.learning.LearningAgent;
import burlap.behavior.singleagent.learning.tdmethods.QLearning;
import burlap.domain.singleagent.gridworld.GridWorldDomain;
import burlap.domain.singleagent.gridworld.GridWorldTerminalFunction;
import burlap.domain.singleagent.gridworld.GridWorldVisualizer;
import burlap.domain.singleagent.gridworld.state.GridAgent;
import burlap.domain.singleagent.gridworld.state.GridLocation;
import burlap.domain.singleagent.gridworld.state.GridWorldState;
import burlap.mdp.singleagent.environment.SimulatedEnvironment;
import burlap.mdp.core.state.State;
import burlap.mdp.singleagent.SADomain;
import burlap.shell.visual.VisualExplorer;
import burlap.statehashing.simple.SimpleHashableStateFactory;
import burlap.visualizer.Visualizer;

public class TMazeGrid {
	
	public static void main(String[] args) {
		GridWorldDomain tmaze = new GridWorldDomain(3,2); //3x2 grid world
		tmaze.verticalWall(1, 1, 0); //left wall
		tmaze.verticalWall(1, 1, 2); //right wall
		tmaze.setTf(new GridWorldTerminalFunction(2,0));
		
		SADomain domain = tmaze.generateDomain(); //generate the grid world domain
		
		//setup initial state
		State s = new GridWorldState(new GridAgent(1, 1), new GridLocation(2, 0, "happy face"));
		
		SimulatedEnvironment env = new SimulatedEnvironment(domain, s);
		//create Q-learning
		LearningAgent agent = new QLearning(domain, 0.99, new SimpleHashableStateFactory(), 
		0.1, 0.1);
		
		//run Q-learning and store results in a list
		List<Episode> episodes = new ArrayList<Episode>(1000);
		for(int i = 0; i < 100; i++){
			episodes.add(agent.runLearningEpisode(env));
			env.resetEnvironment();
		}
		
		//create visualizer and explorer
		Visualizer v = GridWorldVisualizer.getVisualizer(tmaze.getMap());
		VisualExplorer exp = new VisualExplorer(domain, v, s);
		new EpisodeSequenceVisualizer(v, domain, episodes);
		
		exp.initGUI();
		
	}

//	public void QLearningExample(String outputPath){
//		
//		LearningAgent agent = new QLearning(domain, 0.99, hashingFactory, 0., 1.);
//
//		//run learning for 50 episodes
//		for(int i = 0; i < 50; i++){
//			Episode e = agent.runLearningEpisode(env);
//
//			e.write(outputPath + "ql_" + i);
//			System.out.println(i + ": " + e.maxTimeStep());
//
//			//reset environment for next learning episode
//			env.resetEnvironment();
//		}
//		
//	}

}
